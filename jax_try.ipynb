{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://roberttlange.com/posts/2020/03/blog-post-10/ \n",
    "import jax\n",
    "from typing import Any, Callable, Sequence\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, value_and_grad\n",
    "from jax import random\n",
    "\n",
    "# Generate key which is used to generate random numbers\n",
    "key = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.1 ms, sys: 39.3 ms, total: 93.4 ms\n",
      "Wall time: 16.6 ms\n",
      "CPU times: user 20.5 ms, sys: 202 ms, total: 223 ms\n",
      "Wall time: 10.4 ms\n",
      "CPU times: user 151 ms, sys: 1.94 s, total: 2.09 s\n",
      "Wall time: 130 ms\n"
     ]
    }
   ],
   "source": [
    "x = random.uniform(key, (1000, 1000))\n",
    "\n",
    "%time y = np.dot(x,x)\n",
    "\n",
    "%time y = jnp.dot(x,x) #only measures dispatch time, block_until_ready actually requires computation time being factored in \n",
    "%time y = jnp.dot(x,x).block_until_ready()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jax.jit\n",
    "def ReLU(x):\n",
    "    return jnp.maximum(0, x)\n",
    "jit_relu = jit(ReLU)\n",
    "#jit doesn't work on conditioning on dtype or shape, or making compile-time comparisons among static variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 ms, sys: 48 μs, total: 18.1 ms\n",
      "Wall time: 17.9 ms\n",
      "CPU times: user 16.8 ms, sys: 416 μs, total: 17.2 ms\n",
      "Wall time: 16.9 ms\n",
      "CPU times: user 638 μs, sys: 698 μs, total: 1.34 ms\n",
      "Wall time: 390 μs\n"
     ]
    }
   ],
   "source": [
    "#regular relu\n",
    "%time out = ReLU(x)\n",
    "\n",
    "#jnp relu dispatch time\n",
    "%time out = jit_relu(x)\n",
    "\n",
    "#jnp relu actual computation time\n",
    "%time out = jit_relu(x).block_until_ready() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jax Grad: 1.0\n",
      "FD Gradient: 0.99998707\n"
     ]
    }
   ],
   "source": [
    "def FiniteDiffGrad(x):\n",
    "    return jnp.array( (ReLU(x + 1e-3) - ReLU(x - 1e-3) ) / (2 * 1e-3) )\n",
    "\n",
    "#grad is adc \n",
    "#automatically differentiate the jitted ReLU activation, and then jit the gradient function\n",
    "print(\"Jax Grad:\", jit(grad(jit(ReLU)))(2.))\n",
    "#this is default approximated finite diff grad\n",
    "print(\"FD Gradient:\", FiniteDiffGrad(2.) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dim = 64\n",
    "feature_dim = 50\n",
    "hidden_dim = 256\n",
    "\n",
    "X = random.normal(key, (batch_dim, feature_dim))\n",
    "\n",
    "params = [random.normal(key, (hidden_dim, feature_dim)), random.normal(key, (hidden_dim,))]\n",
    "\n",
    "def relu_layer(params, x):\n",
    "    # print(params[0].shape)\n",
    "    # print(x.shape)\n",
    "    # print(params[1].shape)\n",
    "    return ReLU(jnp.dot(params[0], x) + params[1] )\n",
    "#you get output of shape (256,), and you stack it 64 times (which is X.shape[0])\n",
    "\n",
    "def batch_relu_layer(params, X):\n",
    "    return ReLU(jnp.dot(X, params[0].T) + params[1] )\n",
    "\n",
    "def vmap_relu(params, x):\n",
    "    return jit(\n",
    "        vmap(\n",
    "                    #out axes indicates we should stack along first dimension (equivalent to providing X.shape[0] for a jnp.stack)\n",
    "                    relu_layer, in_axes=(None, 0), out_axes=0\n",
    "                )\n",
    "    )\n",
    "    \n",
    "out = jnp.stack([\n",
    "    relu_layer(params, X[i, :]) for i in range(X.shape[0])\n",
    "    ])\n",
    "\n",
    "out.shape # (64, 256)\n",
    "\n",
    "out = batch_relu_layer(params, X)\n",
    "out = vmap_relu(params, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP for MNIST\n",
    "from jax.scipy.special import logsumexp\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from jax.example_libraries import optimizers\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(0.1307, 0.3081)\n",
    "            ])),\n",
    "\n",
    "    batch_size = batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, \n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(0.1307, 0.3081)\n",
    "            ])),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 784) (512,)\n",
      "(512, 512) (512,)\n",
      "(10, 512) (10,)\n"
     ]
    }
   ],
   "source": [
    "def initialize_mlp(sizes, key):\n",
    "    keys = random.split(key, len(sizes))\n",
    "    \n",
    "    #initialize layer with Gauss weights\n",
    "    def initialize_layer(m, n, key, scale=1e-2):\n",
    "        weight_key, bias_key = random.split(key)\n",
    "        return scale * random.normal(weight_key, (n, m)), scale * random.normal(bias_key, (n,))\n",
    "    return [\n",
    "        initialize_layer(m, n,k ) for m, n, k in zip(sizes[:-1], sizes[1:], keys)\n",
    "    ]\n",
    "    \n",
    "layer_sizes = [784, 512, 512, 10]\n",
    "\n",
    "params = initialize_mlp(layer_sizes, key)\n",
    "#read out params shapes\n",
    "for p in params:\n",
    "    print(p[0].shape, p[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(params, in_array):\n",
    "    activations = in_array\n",
    "    for w, b in params[:-1]:\n",
    "        activations = relu_layer([w,b], activations)\n",
    "        \n",
    "    #logits\n",
    "    final_w, final_b = params[-1]\n",
    "    logits = jnp.dot(final_w, activations) + final_b\n",
    "    return logits - logsumexp(logits)\n",
    "\n",
    "batch_forward = vmap(forward_pass, in_axes = (None, 0), out_axes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 1, 24, 8)\n"
     ]
    }
   ],
   "source": [
    "def one_hot(x, key, dtype=jnp.float32):\n",
    "    #one-hot encode x with size k, and expands dimensions to have middle dimension of 1 at dim 2\n",
    "    return jnp.array(x[:, None] == jnp.arange(key), dtype  )\n",
    "\n",
    "\n",
    "#try with tensor x\n",
    "x = random.uniform(key, (30, 24,8))\n",
    "\n",
    "x_ = one_hot(x, 8)\n",
    "print(x_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(params, in_arrays, targets):\n",
    "    preds = batch_forward(params, in_arrays)\n",
    "    #cross entropy loss as sum form (don't specify axis so as to have defined secnod dimension as 1)\n",
    "    return -jnp.sum(preds * targets)\n",
    "\n",
    "def accuracy(params, data_loader): \n",
    "    acc_total = 0\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        images = jnp.array(data).reshape(data.size(0), 784)\n",
    "        #images are shaped as (100, 784)\n",
    "        targets = one_hot(jnp.array(target), 10)\n",
    "        #resulting target reshaped to (100, 10)\n",
    "        target_class = jnp.argmax(targets, axis=1)\n",
    "        #highest value prediction gest the class label\n",
    "        predicted_class = jnp.argmax(batch_forward(params, images), axis=1)\n",
    "        #predicted class as just (100,)\n",
    "        acc_total += jnp.sum(predicted_class == target_class)\n",
    "    return acc_total/len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def update(params, x, y, opt_state): \n",
    "    #eval both loss and gradient of loss (as tuple return)\n",
    "    value, grads = value_and_grad(loss)(params, x, y)\n",
    "    #update optimizer and get new state\n",
    "    opt_state = opt_update(0, grads, opt_state)\n",
    "    return get_params(opt_state), opt_state, value\n",
    "    \n",
    "step_size = 1e-3\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
    "opt_state = opt_init(params)\n",
    "\n",
    "num_epochs = 1\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | T: 5.13 | Train A: 0.971 | Test A: 0.968\n"
     ]
    }
   ],
   "source": [
    "def run_mnist_training_loop(num_epochs, opt_state, net_type=\"MLP\"):\n",
    "    \"\"\" Implements a learning loop over epochs. \"\"\"\n",
    "    # Initialize placeholder for loggin\n",
    "    log_acc_train, log_acc_test, train_loss = [], [], []\n",
    "\n",
    "    # Get the initial set of parameters\n",
    "    params = get_params(opt_state)\n",
    "\n",
    "    # Get initial accuracy after random init\n",
    "    train_acc = accuracy(params, train_loader)\n",
    "    test_acc = accuracy(params, test_loader)\n",
    "    log_acc_train.append(train_acc)\n",
    "    log_acc_test.append(test_acc)\n",
    "\n",
    "    # Loop over the training epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            if net_type == \"MLP\":\n",
    "                # Flatten the image into 784 vectors for the MLP\n",
    "                x = jnp.array(data).reshape(data.size(0), 28*28)\n",
    "            elif net_type == \"CNN\":\n",
    "                # No flattening of the input required for the CNN\n",
    "                x = jnp.array(data)\n",
    "            y = one_hot(jnp.array(target), num_classes)\n",
    "            params, opt_state, loss = update(params, x, y, opt_state)\n",
    "            train_loss.append(loss)\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        train_acc = accuracy(params, train_loader)\n",
    "        test_acc = accuracy(params, test_loader)\n",
    "        log_acc_train.append(train_acc)\n",
    "        log_acc_test.append(test_acc)\n",
    "        print(\"Epoch {} | T: {:0.2f} | Train A: {:0.3f} | Test A: {:0.3f}\".format(epoch+1, epoch_time,\n",
    "                                                                    train_acc, test_acc))\n",
    "\n",
    "    return train_loss, log_acc_train, log_acc_test\n",
    "\n",
    "\n",
    "train_loss, train_log, test_log = run_mnist_training_loop(num_epochs,\n",
    "                                                        opt_state,\n",
    "                                                        net_type=\"MLP\")\n",
    "\n",
    "# # Plot the loss curve over time\n",
    "# from helpers import plot_mnist_performance\n",
    "# plot_mnist_performance(train_loss, train_log, test_log,\n",
    "#                        \"MNIST MLP Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
